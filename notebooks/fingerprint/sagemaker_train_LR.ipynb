{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d903133d",
   "metadata": {},
   "source": [
    "# Fingerprint classification\n",
    "\n",
    "This tutorial was adapted from from [Tensorflow tutorial Image Classification](https://www.tensorflow.org/tutorials/images/classification) and using the [SOCOFing data set from Kaggle](https://www.kaggle.com/datasets/ruizgara/socofing) to demonstrate different dataset, training and tuning strategies.\n",
    "\n",
    "This notebook is designed to run on SageMaker notebook instances with a conda_tensorflow2_p38 kernel.\n",
    "\n",
    "- Examine and understand data\n",
    "- Identifying overfitting and applying techniques to mitigate it, including data augmentation and dropout.\n",
    "- Efficiently loading a dataset off disk.\n",
    "- Build an input pipeline\n",
    "- Build the model\n",
    "- Train the model\n",
    "- Test the model\n",
    "- Improve the model and repeat the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d4b84",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089211a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this directory is apart of the .gitignore to ensure it is not committed to git\n",
    "%env SCRATCH=../scratch\n",
    "![ -e \"${SCRATCH}\" ] || mkdir -p \"${SCRATCH}\"/model\n",
    "\n",
    "import os\n",
    "scratch_path = os.environ.get('SCRATCH', '../scratch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193fb493",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b492230",
   "metadata": {},
   "source": [
    "## Create local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f6fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../scratch/{hand,model}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62063b13",
   "metadata": {},
   "source": [
    "## Extract the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a1779",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xJf ./compressed_data/left.xz -C $scratch_path/hand/ && \\\n",
    "tar -xJf ./compressed_data/right.xz -C ../scratch/hand/ && \\\n",
    "tar -xJf ./compressed_data/real.xz -C ../scratch/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104057a",
   "metadata": {},
   "source": [
    "## Set up AWS client\n",
    "\n",
    "To access any AWS service with Boto3, we have to connect to it with a client. Here, we create an S3 client. We specify the region in which our data lives. We also have to pass the access key and the password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# UPDATE HERE: your environment variables\n",
    "#\n",
    "\n",
    "%env AWS_ACCESS_KEY_ID=update\n",
    "%env AWS_SECRET_ACCESS_KEY=update\n",
    "%env AWS_DEFAULT_REGION=update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861656d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# configure boto S3 connection\n",
    "#\n",
    "\n",
    "import boto3\n",
    "\n",
    "aws_region = os.getenv('AWS_DEFAULT_REGION')                # enter your region name\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')          # enter your S3 access key ID\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')  # enter your S3 secret access key\n",
    "\n",
    "# configure boto S3 connection\n",
    "s3 = boto3.client('s3',\n",
    "                  aws_region,\n",
    "                  aws_access_key_id = aws_access_key_id,\n",
    "                  aws_secret_access_key = aws_secret_access_key)\n",
    "\n",
    "s3.list_buckets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0334ea96",
   "metadata": {},
   "source": [
    "## Create buckets in S3 to move training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new bucket for storing the extracted training data for left hand fingerprints\n",
    "s3.create_bucket(Bucket=\"left-fingerprints\", CreateBucketConfiguration={'LocationConstraint': 'us-east-2'})\n",
    "\n",
    "# create a new bucket for storing the extracted training data for right hand fingerprints\n",
    "s3.create_bucket(Bucket=\"right-fingerprints\", CreateBucketConfiguration={'LocationConstraint': 'us-east-2'})\n",
    "\n",
    "# create a new bucket for storing the extracted training data for real fingerprints\n",
    "s3.create_bucket(Bucket=\"real-fingerprints\", CreateBucketConfiguration={'LocationConstraint': 'us-east-2'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213898cf",
   "metadata": {},
   "source": [
    "## Upload training data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a1d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_files(path):\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "        region_name=os.getenv('AWS_DEFAULT_REGION')\n",
    "    )\n",
    "    s3 = session.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    " \n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(subdir, file)\n",
    "            with open(full_path, 'rb') as data:\n",
    "                bucket.put_object(Key=full_path[len(path)+1:], Body=data)\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    upload_files('path/to/my/folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a995e",
   "metadata": {},
   "source": [
    "### Left fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2783c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'left-fingerprints'\n",
    "upload_files(scratch_path + '/hand/left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a62a13",
   "metadata": {},
   "source": [
    "### Right fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8c2020",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'right-fingerprints'\n",
    "upload_files(scratch_path + '/hand/right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a7f66",
   "metadata": {},
   "source": [
    "### Real fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a82ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'real-fingerprints'\n",
    "upload_files(scratch_path + '/hand/real')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b906563",
   "metadata": {},
   "source": [
    "## Split the data into Train, Validation and Test\n",
    "\n",
    "Keras utility generates a dataset from image files in a directory and infers the labels based on the parent folder. This utility will return a tf.data.Dataset that yields batches of images from the subdirectories left and right\n",
    "\n",
    "```\n",
    "hand/\n",
    "...left/\n",
    "......a_image_1.jpg\n",
    "......a_image_2.jpg\n",
    "...right/\n",
    "......b_image_1.jpg\n",
    "......b_image_2.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb96d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# set variables for consistency\n",
    "# \n",
    "\n",
    "img_height = 96              # desired height\n",
    "img_width = 96               # desiredd width\n",
    "batch_size = 32              # batch inputs in 32\n",
    "seed_train_validation = 42   # Must be same for train_ds and val_ds\n",
    "validation_split = 0.3       # move 30% of the data into validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd733a6",
   "metadata": {},
   "source": [
    "### Create Train\n",
    "\n",
    "Train is the sample of data used to fit the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f74933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '../scratch/hand',\n",
    "    labels='inferred',\n",
    "    label_mode = \"categorical\", \n",
    "    class_names=['left','right'],\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True, \n",
    "    seed=seed_train_validation,\n",
    "    validation_split=validation_split,\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f050970a",
   "metadata": {},
   "source": [
    "### Create Validation\n",
    "\n",
    "Validation is the sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '../scratch/hand',\n",
    "    labels='inferred',\n",
    "    label_mode = \"categorical\", \n",
    "    class_names=['left','right'],\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True, \n",
    "    seed=seed_train_validation,\n",
    "    validation_split=validation_split,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82c281",
   "metadata": {},
   "source": [
    "## Create Test\n",
    "\n",
    "The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd96222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the validation_ds into validation and test data\n",
    "test_ds = validation_ds.take(16)\n",
    "validation_ds = validation_ds.skip(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d9c9f5",
   "metadata": {},
   "source": [
    "## Print the Dataset batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserves 393 batches training\n",
    "print('70% of data in batches of 32 images for training -->', train_ds.cardinality())\n",
    "# reserves 164 batches validation\n",
    "print('20% of data in batches of 32 images for validating -->', validation_ds.cardinality())\n",
    "# reserves 5 batches testing\n",
    "print('10% of data in batches of 32 images for testing -->', test_ds.cardinality())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e179d3",
   "metadata": {},
   "source": [
    "## Print Inferred Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e7c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the class names inferred from the training dataset\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c20166d",
   "metadata": {},
   "source": [
    "## Print Fingerprint Data Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adbd61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first 10 images in the training dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(10):\n",
    "    ax = plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"), cmap='gray')\n",
    "    #TODO update labels\n",
    "    #plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c7699e",
   "metadata": {},
   "source": [
    "## Apply augmentation\n",
    "\n",
    "When you don't have a large image dataset or when your images are all set in a single direction like ours are, it's a good practice to artificially introduce sample diversity by applying random, yet realistic, transformations to the training images, such as rotation and horizontal flipping. This helps expose the model to different aspects of the training data and reduce overfitting. Learn more https://www.tensorflow.org/tutorials/images/data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e93291",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "\n",
    "  # randomly rotates images during training\n",
    "  tf.keras.layers.RandomRotation(\n",
    "    # a float represented as fraction of 2 Pi, or a tuple of size 2 representing lower and upper bound for rotating clockwise and counter-clockwise. \n",
    "    # A positive values means rotating counter clock-wise, while a negative value means clock-wise. \n",
    "    0.2,\n",
    "      \n",
    "    # Points outside the boundaries of the input are filled according to the given mode (one of {\"constant\", \"reflect\", \"wrap\", \"nearest\"}).\n",
    "    fill_mode='constant',\n",
    "      \n",
    "    # Supported values: \"nearest\", \"bilinear\".\n",
    "    interpolation='nearest',\n",
    "      \n",
    "    # Integer. Used to create a random seed.\n",
    "    seed=None,\n",
    "      \n",
    "    # the value to be filled outside the boundaries when fill_mode=\"constant\".\n",
    "    fill_value=0.0,\n",
    "),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a748ba6",
   "metadata": {},
   "source": [
    "Visualize a few augmented examples by applying data augmentation to the same image several times:Visualize a few augmented examples by applying data augmentation to the same image several times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b89acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, _ in train_ds.take(1):\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  first_image = image[2]\n",
    "  for i in range(10):\n",
    "    ax = plt.subplot(5, 5, i + 1)\n",
    "    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "    plt.imshow(augmented_image[0] / 1, cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec6df2",
   "metadata": {},
   "source": [
    "## Configure the datasets for performance\n",
    "\n",
    "Let's make sure to use buffered prefetching so we can yield data from disk without having I/O become blocking. These are two important methods you should use when loading data.\n",
    "\n",
    "1. `Caching` a dataset, either in memory or on local storage. This will save some operations (like file opening and data reading) from being executed during each epoch.\n",
    "1. `Prefetching` overlaps the preprocessing and model execution of a training step. While the model is executing training step s, the input pipeline is reading the data for step s+1. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37bc48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "validation_ds = validation_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = validation_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aa120b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6650a17",
   "metadata": {},
   "source": [
    "## Define a training strategy\n",
    "\n",
    "tf.distribute.Strategy is a TensorFlow API to distribute training across multiple GPUs, multiple machines, or TPUs. Using this API, you can distribute your existing models and training code with minimal code changes.\n",
    "- Easy to use and support multiple user segments, including researchers, machine learning engineers, etc.\n",
    "- Provide good performance out of the box.\n",
    "- Easy switching between strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ed9ca",
   "metadata": {},
   "source": [
    "### Mirrored Strategy \n",
    "\n",
    "Supports synchronous distributed training on multiple GPUs on one machine. It creates one replica per GPU device. Each variable in the model is mirrored across all the replicas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda4949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# uncomment to reate a MirroredStrategy.\n",
    "# \n",
    "\n",
    "#strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "#strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a45f5",
   "metadata": {},
   "source": [
    "### Multi-Worker Mirrored Strategy\n",
    "\n",
    "Multi-Worker MirroredStrategy is very similar to MirroredStrategy. It implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to tf.distribute.MirroredStrategy, it creates copies of all variables in the model on each device across all workers.\n",
    "\n",
    "MultiWorkerMirroredStrategy has two implementations for cross-device communications. \n",
    "1. CommunicationImplementation.RING is RPC-based and supports both CPUs and GPUs. \n",
    "1. CommunicationImplementation.NCCL uses NCCL and provides state-of-art performance on GPUs but it doesn't support CPUs. \n",
    "1. CollectiveCommunication.AUTO defers the choice to Tensorflow. You can specify them in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b673caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# uncomment to create a Multi-worker Mirrored Strategy\n",
    "#\n",
    "\n",
    "communication_options = tf.distribute.experimental.CommunicationOptions(\n",
    "    # RING is RPC-based and supports both CPUs and GPUs.\n",
    "    #implementation=tf.distribute.experimental.CommunicationImplementation.RING)\n",
    "\n",
    "    # NCCL uses NCCL and provides state-of-art performance on GPUs but it doesn't support CPUs\n",
    "    #implementation=tf.distribute.experimental.CommunicationImplementation.NCCL)\n",
    "    \n",
    "    # AUTO defers the choice to Tensorflow.\n",
    "    implementation=tf.distribute.experimental.CommunicationImplementation.AUTO)\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=communication_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9cad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Understand the number of devices and GPU available\n",
    "#\n",
    "\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88fe6ca",
   "metadata": {},
   "source": [
    "## Define a model and hyperparameters\n",
    "\n",
    "When you build a model for hypertuning, you also define the hyperparameter search space in addition to the model architecture. The model you set up for hypertuning is called a hypermodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb97134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# A model definition that doesn't take advantage of a training strategy\n",
    "#\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout,Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "\n",
    "num_classes = len(class_names)\n",
    "model_path = scratch_path + '/model'\n",
    "\n",
    "inputShape=(img_height, img_width, 1)\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    data_augmentation\n",
    "    input_shape=(img_height, img_width, 1)\n",
    "    chanDim = -1\n",
    "    # first CONV => RELU => POOL layer set\n",
    "    model.add(Conv2D(\n",
    "        hp.Int(\"conv_1\", min_value=32, max_value=96, step=32),\n",
    "        (3, 3), padding=\"same\", input_shape=inputShape, data_format=\"channels_last\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # second CONV => RELU => POOL layer set\n",
    "    model.add(Conv2D(\n",
    "        hp.Int(\"conv_2\", min_value=64, max_value=128, step=32),\n",
    "        (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # third CONV => RELU => POOL layer set\n",
    "    model.add(Conv2D(\n",
    "        hp.Int(\"conv_3\", min_value=96, max_value=256, step=32),\n",
    "        (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))    \n",
    "    \n",
    "    # first (and only) set of FC => RELU layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hp.Int(\"dense_units\", min_value=256,\n",
    "                           max_value=768, step=256)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    # softmax classifier\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    # initialize the learning rate choices and optimizer\n",
    "    lr = hp.Choice(\"learning_rate\",\n",
    "                   values=[1e-1, 1e-2, 1e-3])\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    # return the model\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        # metrics to be evaluated by the model during training and testing.The strings 'accuracy' or 'acc', TF converts this to binary, categorical or sparse.\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16b28b8",
   "metadata": {},
   "source": [
    "## Search hyperparameters\n",
    "\n",
    "The Keras Tuner has four tuners available:\n",
    "1. RandomSearch\n",
    "1. Hyperband\n",
    "1. BayesianOptimization\n",
    "1. Sklearn. \n",
    "\n",
    "In this tutorial, you use the Hyperband tuner. The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. \n",
    "\n",
    "To instantiate the Hyperband tuner, you must specify the hypermodel, the objective to optimize and the maximum number of epochs to train (max_epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a736edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "model_path = scratch_path + '/model'\n",
    "\n",
    "# Open a strategy scope.\n",
    "tuner = kt.Hyperband(\n",
    "    model_builder,\n",
    "    objective='val_accuracy',\n",
    "    # nteger, the maximum number of epochs to train one model. It is recommended to set this to a value slightly higher than the expected epochs to convergence for your largest Model, and to use early stopping during trainin\n",
    "    max_epochs=2,\n",
    "    # Integer, the reduction factor for the number of epochs and number of models for each bracket. Defaults to 3.\n",
    "    factor=3,\n",
    "    # training strategy\n",
    "    distribution_strategy=strategy,\n",
    "    directory=scratch_path + '/model/model_hp',\n",
    "    project_name='hypertune',\n",
    "    #  If you re-run the hyperparameter search, the Keras Tuner uses the existing state from these logs to resume the search. \n",
    "    # To disable this behavior, pass an additional overwrite=True argument while instantiating the tuner.\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12faf8",
   "metadata": {},
   "source": [
    "We’ll be using EarlyStopping to short circuit hyperparameter trials that are not performing well. Keep in mind that tuning hyperparameters is an extremely computationally expensive process, so if we can kill off poorly performing trials, we can save ourselves a bunch of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e88ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop training when a monitored metric has stopped improving.\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b0fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_ds, epochs=4, validation_data=validation_ds, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"[INFO] optimal number of filters in conv_1 layer: {}\".format(\n",
    "\tbest_hps.get(\"conv_1\")))\n",
    "print(\"[INFO] optimal number of filters in conv_2 layer: {}\".format(\n",
    "\tbest_hps.get(\"conv_2\")))\n",
    "print(\"[INFO] optimal number of filters in conv_2 layer: {}\".format(\n",
    "\tbest_hps.get(\"conv_3\")))\n",
    "print(\"[INFO] optimal number of units in dense layer: {}\".format(\n",
    "\tbest_hps.get(\"dense_units\")))\n",
    "print(\"[INFO] optimal learning rate: {:.4f}\".format(\n",
    "\tbest_hps.get(\"learning_rate\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b91ead",
   "metadata": {},
   "source": [
    "## Fit a model\n",
    "\n",
    "Fit the model with the optimal hyperparameters and train it on the data for 50 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88925c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=validation_ds,\n",
    "    epochs=epochs,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False\n",
    ")\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3ba56",
   "metadata": {},
   "source": [
    "## Print the model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c3ef9b",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8674afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(\n",
    "    test_ds,\n",
    "    batch_size=batch_size,\n",
    "    verbose='auto',\n",
    "    sample_weight=None,\n",
    "    steps=None,\n",
    "    callbacks=None,\n",
    "    max_queue_size=10,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe4249",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "\n",
    "You can save an entire model to a single artifact. It will include:\n",
    "\n",
    "- The model's architecture/config\n",
    "- The model's weight values (which were learned during training)\n",
    "- The model's compilation information (if compile() was called)\n",
    "- The optimizer and its state, if any (this enables you to restart training where you left)\n",
    "\n",
    "There are two formats you can use to save an entire model to disk: the TensorFlow SavedModel format, and the older Keras H5 format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dbdc54",
   "metadata": {},
   "source": [
    "## Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36539eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# older Keras H5 format\n",
    "model.save( scratch_path + '/model/hand_prediction.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b256bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow SavedModel format\n",
    "model.save( scratch_path + '/model/hand_prediction.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60023558",
   "metadata": {},
   "source": [
    "## S3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc59007",
   "metadata": {},
   "source": [
    "# Make a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc36c87",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e90e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model( scratch_path + '/model/hand_prediction.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f046e7",
   "metadata": {},
   "source": [
    "## Load a new fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35194285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# un/comment test a left finger\n",
    "#path = scratch_path + '/real/10__M_Left_thumb_finger.png'\n",
    "\n",
    "# un/comment test a right finger\n",
    "path = scratch_path + '/real/1__M_Right_ring_finger.png'\n",
    "\n",
    "\n",
    "# Loads an image into PIL format.\n",
    "img = tf.keras.utils.load_img(\n",
    "    path,\n",
    "    color_mode='grayscale',\n",
    "    target_size=(img_height, img_width),\n",
    "    interpolation='nearest',\n",
    "    keep_aspect_ratio=False\n",
    ")\n",
    "\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc589378",
   "metadata": {},
   "source": [
    "## Convert the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3a384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a PIL Image instance to a Numpy array.\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "\n",
    "# Returns a tensor with a length 1 axis inserted at index axis.\n",
    "#img_array = tf.expand_dims(img_array, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcc51c6",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c084f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a prediction on the new fingerprint\n",
    "predictions = model.predict(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fa656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e383a6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
