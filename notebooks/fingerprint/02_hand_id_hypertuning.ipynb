{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d0b974",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ec9d98",
   "metadata": {},
   "source": [
    "This tutorial shows how to classify images from fingerprints to originating from a left or right hand using a tf.keras.Sequential model .\n",
    "\n",
    "1. Setup\n",
    "1. Load the data\n",
    "1. Build and split the data into Train, Validation and Test\n",
    "1. Label the datasets\n",
    "1. Apply Augmentation to the datasets\n",
    "1. Optimize the dataset\n",
    "1. Define the model structure, a training strategy, hyperparameter tuning, and compile\n",
    "1. Perform a Random Search over Hyperparameters\n",
    "1. Train and Hypertune model\n",
    "1. Evaluate the model\n",
    "1. Predict on new data\n",
    "1. Save the model\n",
    "\n",
    "In addition, the _compression notebook demonstrates how to convert a saved model to a TensorFlow Lite model for on-device machine learning on mobile, embedded, and IoT devices.\n",
    "\n",
    "References: \n",
    "1. https://www.kaggle.com/datasets/ruizgara/socofing\n",
    "1. https://www.tensorflow.org/tutorials/images/classification\n",
    "1. https://pyimagesearch.com/2021/06/07/easy-hyperparameter-tuning-with-keras-tuner-and-tensorflow/\n",
    "1. https://keras.io/guides/distributed_training/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab3f8c6",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ed718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quiet install of requirements\n",
    "!pip install -U pip tensorflow -q\n",
    "!pip install -r ../../requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dd24ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout,Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "print(\"tensorflow version:  \" + tf.__version__)\n",
    "\n",
    "\n",
    "# '0' Log all messages.\n",
    "# '1' Log all messages except INFO.\n",
    "# '2' Log all messages except INFO and WARNING. (default)\n",
    "# '3' Log all messages except INFO, WARNING, and ERROR.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c4613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this directory is apart of the .gitignore to ensure it is not committed to git\n",
    "%env SCRATCH=../scratch\n",
    "![ -e \"${SCRATCH}\" ] || mkdir -p \"${SCRATCH}\"/model\n",
    "\n",
    "import os\n",
    "scratch_path = os.environ.get('SCRATCH', '../scratch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07226e3",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf490cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../scratch/{hand,model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb6a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xJf ./compressed_data/left.xz -C ../scratch/hand/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce5f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xJf ./compressed_data/right.xz -C ../scratch/hand/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974066f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xJf ./compressed_data/real.xz -C ../scratch/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035bbf43",
   "metadata": {},
   "source": [
    "# Split the data into Train, Validation and Test\n",
    "\n",
    "Use .take() and .skip() to further split the validation_ds set into 2 datasets -- one for validation and the other for test. Let's assume that you need 80% for training set, 10% for validation set, and 10% for test set. Determine how many batches of data are available in the validation set using tf.data.experimental.cardinality, and then move the two-third of them (2/3 of 30% = 20%) to a test set as follows. Note that the default value of batch_size is 32\n",
    "\n",
    "All the three datasets (train_ds, val_ds, and test_ds) yield batches of images together with labels inferred from the directory structure.\n",
    "\n",
    "- Training Dataset: The sample of data used to fit the model.\n",
    "- Validation Dataset: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.\n",
    "- Test Dataset: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.# Create training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters for the loader\n",
    "\n",
    "img_height = 96\n",
    "img_width = 96\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b288c91c",
   "metadata": {},
   "source": [
    "It's good practice to use a validation split when developing your model. We will use 80% of the images for training, and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439a969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    # Directory where the data is located. If labels is \"inferred\", it should contain subdirectories, each containing images for a class. \n",
    "    '../scratch/hand',\n",
    "    \n",
    "    # Either \"inferred\" (labels are generated from the directory structure), None (no labels), or a list/tuple of integer labels of the same size as the number of image files found in the directory. \n",
    "    labels='inferred',\n",
    "    \n",
    "    # String describing the encoding of labels. 'int': means that the labels are encoded as integers. (e.g. for sparse_categorical_crossentropy loss).\n",
    "    # 'categorical' means that the labels are encoded as a categorical vector.  (e.g. for categorical_crossentropy loss).\n",
    "    #'binary' means that the labels (there can be only 2) are encoded as float32 scalars with values 0 or 1. (e.g. for binary_crossentropy).\n",
    "    label_mode = \"categorical\", \n",
    "    \n",
    "    # Only valid if \"labels\" is \"inferred\". This is the explicit list of class names (must match names of subdirectories).\n",
    "    class_names=['left','right'],\n",
    "    \n",
    "    # One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\". Whether the images will be converted to have 1, 3, or 4 channels.\n",
    "    color_mode=\"grayscale\",\n",
    "    \n",
    "    # Size of the batches of data. Default: 32. If None, the data will not be batched.\n",
    "    batch_size=batch_size,\n",
    "    \n",
    "    # Size to resize images to after they are read from disk, specified as (height, width). Defaults to (256, 256).\n",
    "    image_size=(img_height, img_width),\n",
    "    \n",
    "    # Whether to shuffle the data. Default: True. If set to False, sorts the data in alphanumeric order.\n",
    "    shuffle=True, \n",
    "    \n",
    "    # Optional random seed for shuffling and transformations.\n",
    "    seed=42,\n",
    "    \n",
    "    # Optional float between 0 and 1, fraction of data to reserve for validation.\n",
    "    validation_split=0.2,\n",
    "    \n",
    "    # Subset of the data to return. One of \"training\", \"validation\" or \"both\". Only used if validation_split is set. When subset=\"both\", the utility returns a tuple of two datasets\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    # Directory where the data is located. If labels is \"inferred\", it should contain subdirectories, each containing images for a class. \n",
    "    '../scratch/hand',\n",
    "    \n",
    "    # Either \"inferred\" (labels are generated from the directory structure), None (no labels), or a list/tuple of integer labels of the same size as the number of image files found in the directory. \n",
    "    labels='inferred',\n",
    "    \n",
    "    # String describing the encoding of labels. 'int': means that the labels are encoded as integers. (e.g. for sparse_categorical_crossentropy loss).\n",
    "    # 'categorical' means that the labels are encoded as a categorical vector.  (e.g. for categorical_crossentropy loss).\n",
    "    #'binary' means that the labels (there can be only 2) are encoded as float32 scalars with values 0 or 1. (e.g. for binary_crossentropy).\n",
    "    label_mode = \"categorical\", \n",
    "    \n",
    "    # Only valid if \"labels\" is \"inferred\". This is the explicit list of class names (must match names of subdirectories).\n",
    "    class_names=['left','right'],\n",
    "    \n",
    "    # One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\". Whether the images will be converted to have 1, 3, or 4 channels.\n",
    "    color_mode=\"grayscale\",\n",
    "    \n",
    "    # Size of the batches of data. Default: 32. If None, the data will not be batched.\n",
    "    batch_size=batch_size,\n",
    "    \n",
    "    # Size to resize images to after they are read from disk, specified as (height, width). Defaults to (256, 256).\n",
    "    image_size=(img_height, img_width),\n",
    "    \n",
    "    # Whether to shuffle the data. Default: True. If set to False, sorts the data in alphanumeric order.\n",
    "    shuffle=True, \n",
    "    \n",
    "    # Optional random seed for shuffling and transformations.\n",
    "    seed=42,\n",
    "    \n",
    "    # Optional float between 0 and 1, fraction of data to reserve for validation.\n",
    "    validation_split=0.2,\n",
    "    \n",
    "    # Subset of the data to return. One of \"training\", \"validation\" or \"both\". Only used if validation_split is set. When subset=\"both\", the utility returns a tuple of two datasets\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# splits the validation_ds into validation and test data\n",
    "test_ds = validation_ds.take(5)\n",
    "validation_ds = validation_ds.skip(5)\n",
    "\n",
    "# reserves 393 batches training\n",
    "print('Batches for training -->', train_ds.cardinality())\n",
    "# reserves 164 batches validation\n",
    "print('Batches for validating -->', validation_ds.cardinality())\n",
    "# reserves 5 batches testing\n",
    "print('Batches for testing -->', test_ds.cardinality())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7489722b",
   "metadata": {},
   "source": [
    "# Display the inferred class names from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7061971b",
   "metadata": {},
   "source": [
    "You can find the class names in the class_names attribute on these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937fc8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the class names inferred from the training dataset\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be72a2",
   "metadata": {},
   "source": [
    "# Visualize the dataset images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda1bec3",
   "metadata": {},
   "source": [
    "Here are the first 9 images from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e81f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first 9 images in the training dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"), cmap='gray')\n",
    "    #TODO update labels\n",
    "    #plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed467397",
   "metadata": {},
   "source": [
    "The image_batch is a tensor of the shape (32, 96, 96, 1). This is a batch of 32 images of shape 96x96x1 (the last dimension refers to color channels grayscaled). The label_batch is a tensor of the shape (32,), these are corresponding labels to the 32 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e815838",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d9af1d",
   "metadata": {},
   "source": [
    "# Apply Data Augmentation\n",
    "\n",
    "When you don't have a large image dataset or when your images are all set in a single direction like ours are, it's a good practice to artificially introduce sample diversity by applying random, yet realistic, transformations to the training images, such as rotation and horizontal flipping. This helps expose the model to different aspects of the training data and reduce overfitting. Learn more https://www.tensorflow.org/tutorials/images/data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f012d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  # randomly flips images during training\n",
    "  tf.keras.layers.RandomFlip(\n",
    "      # String indicating which flip mode to use. Can be \"horizontal\", \"vertical\", or \"horizontal_and_vertical\"\n",
    "      'horizontal_and_vertical',\n",
    "      \n",
    "      # Integer. Used to create a random seed.\n",
    "      seed=None\n",
    "  ),\n",
    "    \n",
    "  # randomly rotates images during training\n",
    "  tf.keras.layers.RandomRotation(\n",
    "    # a float represented as fraction of 2 Pi, or a tuple of size 2 representing lower and upper bound for rotating clockwise and counter-clockwise. \n",
    "    # A positive values means rotating counter clock-wise, while a negative value means clock-wise. \n",
    "    0.2,\n",
    "      \n",
    "    # Points outside the boundaries of the input are filled according to the given mode (one of {\"constant\", \"reflect\", \"wrap\", \"nearest\"}).\n",
    "    fill_mode='constant',\n",
    "      \n",
    "    # Supported values: \"nearest\", \"bilinear\".\n",
    "    interpolation='nearest',\n",
    "      \n",
    "    # Integer. Used to create a random seed.\n",
    "    seed=None,\n",
    "      \n",
    "    # the value to be filled outside the boundaries when fill_mode=\"constant\".\n",
    "    fill_value=0.0,\n",
    "),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f945e5b",
   "metadata": {},
   "source": [
    "Visualize a few augmented examples by applying data augmentation to the same image several times:Visualize a few augmented examples by applying data augmentation to the same image several times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79faadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, _ in train_ds.take(1):\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  first_image = image[4]\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "    plt.imshow(augmented_image[0] / 1, cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9d3692",
   "metadata": {},
   "source": [
    "# Configure the dataset for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2ccd7d",
   "metadata": {},
   "source": [
    "Configure the dataset for performance. Let's make sure to use buffered prefetching so we can yield data from disk without having I/O become blocking. These are two important methods you should use when loading data.\n",
    "\n",
    ".cache() keeps the images in memory after they're loaded off disk during the first epoch\n",
    "- caching a dataset, either in memory or on local storage. This will save some operations (like file opening and data reading) from being executed during each epoch.\n",
    "\n",
    ".prefetch() overlaps data preprocessing and model execution while training.\n",
    "- Prefetching overlaps the preprocessing and model execution of a training step. While the model is executing training step s, the input pipeline is reading the data for step s+1. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5848efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "validation_ds = validation_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db81188d",
   "metadata": {},
   "source": [
    "# Define the training strategy, Hyperparameter Tune and Compile the model\n",
    "\n",
    "For this tutorial, choose the tf.keras.optimizers.Adam optimizer and tf.keras.losses.SparseCategoricalCrossentropy loss function. To view training and validation accuracy for each training epoch, pass the metrics argument to Model.compile.\n",
    "\n",
    "To do single-host, multi-device synchronous training with a Keras model, you would use the tf.distribute.MirroredStrategy API. Here's how it works:\n",
    "\n",
    "- Instantiate a MirroredStrategy, optionally configuring which specific devices you want to use (by default the strategy will use all GPUs available).\n",
    "- Use the strategy object to open a scope, and within this scope, create all the Keras objects you need that contain variables. Typically, that means creating & compiling the model inside the distribution scope.\n",
    "- Train the model via fit() as usual.\n",
    "\n",
    "When you build a model for hypertuning, you also define the hyperparameter search space in addition to the model architecture. The model you set up for hypertuning is called a hypermodel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10707080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce1a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(img_height, img_width, 1)\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    data_augmentation\n",
    "    # first CONV => RELU => POOL layer set\n",
    "    model.add(Conv2D(\n",
    "        # we define our first hyperparameter to search over — the number of filters in our CONV layer\n",
    "        # The hyperparameter is given a name, conv_1, and can accept values in the range [32, 96] with steps of 32. \n",
    "        # This implies that valid values for conv_1 are 32, 64, 96\n",
    "        data_format=\"channels_last\", hp.Int(\"conv_1\", min_value=16, max_value=64, step=32),\n",
    "        (3, 3), padding=\"same\", input_shape=input_shape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # second CONV => RELU => POOL layer set\n",
    "    model.add(Conv2D(\n",
    "        # For our second CONV layer, we’re allowing more filters to be learned in the range [64, 128]. \n",
    "        # With a step size of 32, this implies that we’ll be testing values of 64, 96, 128\n",
    "        hp.Int(\"conv_2\", min_value=32, max_value=96, step=32),\n",
    "        (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    #model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # third CONV => RELU => POOL layer set\n",
    "    model.add(Conv2D(\n",
    "        # For our second CONV layer, we’re allowing more filters to be learned in the range [64, 128]. \n",
    "        # With a step size of 32, this implies that we’ll be testing values of 64, 96, 128\n",
    "        hp.Int(\"conv_3\", min_value=64, max_value=128, step=32),\n",
    "        (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    #model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # first (and only) set of FC => RELU layers\n",
    "    #  We want to tune the number of nodes in this layer. We specify a minimum of 256 and a maximum of 768 nodes, allowing a step of 256\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hp.Int(\"dense_units\", min_value=128,\n",
    "                           max_value=768, step=256)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Dropout(0.5))\n",
    "    # softmax classifier\n",
    "    #Apply a tf.keras.layers.Dense layer to convert these features into a single prediction per image.\n",
    "    model.add(Dense(num_classes))\n",
    "    # softmax results in this model performing around 50% accuracy, commented out\n",
    "    #model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    # initialize the learning rate choices and optimizer\n",
    "    # For our learning rate, we wish to see which of 1e-1, 1e-2, and 1e-3 performs best. \n",
    "    # Using hp.Choice will allow our hyperparameter tuner to select the best learning rate.\n",
    "    lr = hp.Choice(\"learning_rate\",\n",
    "                   values=[1e-1, 1e-2, 1e-3])\n",
    "    #opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        # optimizers are necessary for your model as they improve training speed and performance. Optional optimizers: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        # Loss function to calculate the models performance. The lower the loss, the closer our predictions are to the true labels.\n",
    "        loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "        # metrics to be evaluated by the model during training and testing.The strings 'accuracy' or 'acc', TF converts this to binary, categorical or sparse.\n",
    "        metrics=['accuracy'],\n",
    "          tf.keras.metrics.BinaryAccuracy(),\n",
    "          tf.keras.metrics.FalseNegatives(),\n",
    "        run_eagerly=None,\n",
    "        # Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. \n",
    "        steps_per_execution=1,\n",
    "    )\n",
    "    # return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda8d33",
   "metadata": {},
   "source": [
    "# Instantiate the tuner and perform hypertuning\n",
    "\n",
    "The Keras Tuner has four tuners available:\n",
    "1. RandomSearch\n",
    "1. Hyperband\n",
    "1. BayesianOptimization\n",
    "1. Sklearn. \n",
    "\n",
    "In this tutorial, you use the Hyperband tuner. The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. \n",
    "\n",
    "To instantiate the Hyperband tuner, you must specify the hypermodel, the objective to optimize and the maximum number of epochs to train (max_epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14edbbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synchronous training across multiple replicas on one machine.\n",
    "# a Keras model that was designed to run on a single-worker can seamlessly work on multiple workers with minimal code changes.\n",
    "# https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras\n",
    "strategy = tf.distribute.MirroredStrategy() # This strategy is typically used for training on one machine with multiple GPUs.\n",
    "#strategy = tf.distribute.MultiWorkerMirroredStrategy(\n",
    "#    cluster_resolver=None, \n",
    "#    communication_options=None) # A distribution strategy for synchronous training on multiple workers.\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "\n",
    "# A model definition that doesn't take advantage of a training strategy\n",
    "\n",
    "num_classes = len(class_names)\n",
    "model_path = scratch_path + '/model'\n",
    "\n",
    "# Open a strategy scope.\n",
    "tuner = kt.Hyperband(\n",
    "    model_builder,\n",
    "    objective='val_accuracy',\n",
    "    # nteger, the maximum number of epochs to train one model. It is recommended to set this to a value slightly higher than the expected epochs to convergence for your largest Model, and to use early stopping during trainin\n",
    "    max_epochs=2,\n",
    "    # Integer, the reduction factor for the number of epochs and number of models for each bracket. Defaults to 3.\n",
    "    factor=3,\n",
    "    # training strategy\n",
    "    distribution_strategy=strategy,\n",
    "    directory=scratch_path + '/model/model_hp',\n",
    "    project_name='hypertune',\n",
    "    #  If you re-run the hyperparameter search, the Keras Tuner uses the existing state from these logs to resume the search. \n",
    "    # To disable this behavior, pass an additional overwrite=True argument while instantiating the tuner.\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f6d2c4",
   "metadata": {},
   "source": [
    "We’ll be using EarlyStopping to short circuit hyperparameter trials that are not performing well. Keep in mind that tuning hyperparameters is an extremely computationally expensive process, so if we can kill off poorly performing trials, we can save ourselves a bunch of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd9581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop training when a monitored metric has stopped improving.\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "    # Quantity to be monitored.\n",
    "    monitor='val_loss', \n",
    "    # Number of epochs with no improvement after which training will be stopped.\n",
    "    patience=5,\n",
    "    # training will stop when the quantity monitored has stopped decreasing \"min\" or increasing \"max\" or auto\n",
    "    model=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a3e193",
   "metadata": {},
   "source": [
    "Run the hyperparameter search. The arguments for the search method are the same as those used for tf.keras.model.fit in addition to the callback above.Run the hyperparameter search. The arguments for the search method are the same as those used for tf.keras.model.fit in addition to the callback above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0240bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_ds, epochs=4, validation_data=validation_ds, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"[INFO] optimal number of filters in conv_1 layer: {}\".format(\n",
    "\tbest_hps.get(\"conv_1\")))\n",
    "print(\"[INFO] optimal number of filters in conv_2 layer: {}\".format(\n",
    "\tbest_hps.get(\"conv_2\")))\n",
    "print(\"[INFO] optimal number of filters in conv_2 layer: {}\".format(\n",
    "\tbest_hps.get(\"conv_3\")))\n",
    "print(\"[INFO] optimal number of units in dense layer: {}\".format(\n",
    "\tbest_hps.get(\"dense_units\")))\n",
    "print(\"[INFO] optimal learning rate: {:.4f}\".format(\n",
    "\tbest_hps.get(\"learning_rate\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a481549",
   "metadata": {},
   "source": [
    "View all the layers of the network using the Keras Model.summary method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d34422e",
   "metadata": {},
   "source": [
    "# Train a model\n",
    "\n",
    "Train the model for 10 epochs with the Keras Model.fit method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e06c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    # x: Input data and y: Target data\n",
    "    train_ds,\n",
    "        \n",
    "    # Number of samples per gradient update. If unspecified, batch_size will default to 32.\n",
    "    batch_size=batch_size,\n",
    "        \n",
    "    # Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. \n",
    "    validation_data=validation_ds,\n",
    "        \n",
    "    # Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided\n",
    "    epochs=epochs,\n",
    "        \n",
    "    # Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1.\n",
    "    workers=1,\n",
    "        \n",
    "    # If True, use process-based threading. If unspecified, use_multiprocessing will default to False.  \n",
    "    use_multiprocessing=False\n",
    ")\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a919cb",
   "metadata": {},
   "source": [
    "# Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ac411",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(\n",
    "    # x: Input data and y: Target data\n",
    "    test_ds,\n",
    "    \n",
    "    # Integer or None. Number of samples per batch of computation.\n",
    "    batch_size=batch_size,\n",
    "    \n",
    "    # \"auto\", 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line.\n",
    "    verbose='auto',\n",
    "    \n",
    "    # Optional Numpy array of weights for the test samples, used for weighting the loss function. \n",
    "    sample_weight=None,\n",
    "    \n",
    "    # Integer or None. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None. \n",
    "    steps=None,\n",
    "    \n",
    "    # List of callbacks to apply during evaluation. See callbacks.\n",
    "    callbacks=None,\n",
    "    \n",
    "    # Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10.\n",
    "    max_queue_size=10,\n",
    "    \n",
    "    # Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1.\n",
    "    workers=1,\n",
    "    \n",
    "    # If True, use process-based threading. If unspecified, use_multiprocessing will default to False. \n",
    "    use_multiprocessing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8466b80c",
   "metadata": {},
   "source": [
    "# Visualize the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d170aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bf1bba",
   "metadata": {},
   "source": [
    "# Predict on new data\n",
    "\n",
    "Use your model to classify an image that wasn't included in the training or validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475fa08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# un/comment test a left finger\n",
    "path = scratch_path + '/real/10__M_Left_thumb_finger.png'\n",
    "\n",
    "# un/comment test a right finger\n",
    "#path = scratch_path + '/real/1__M_Right_ring_finger.png'\n",
    "\n",
    "# Loads an image into PIL format.\n",
    "img = tf.keras.utils.load_img(\n",
    "    path,\n",
    "    color_mode='grayscale',\n",
    "    target_size=(img_height, img_width),\n",
    "    interpolation='nearest',\n",
    "    keep_aspect_ratio=False\n",
    ")\n",
    "\n",
    "\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f050e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a PIL Image instance to a Numpy array.\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "\n",
    "# Returns a tensor with a length 1 axis inserted at index axis.\n",
    "img_array = tf.expand_dims(img_array, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ff4606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a prediction on the new fingerprint\n",
    "predictions = model.predict(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57990588",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a4d5e",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "\n",
    "There are two formats you can use to save an entire model to disk: the TensorFlow SavedModel format, and the older Keras H5 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94dfe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# older Keras H5 format\n",
    "model.save( scratch_path + '/model/hand_prediction.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow SavedModel format\n",
    "model.save( scratch_path + '/model/hand_prediction.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b34b7",
   "metadata": {},
   "source": [
    "# Cleanup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41427324",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ../scratch/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754fd80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
