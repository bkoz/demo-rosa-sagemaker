{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d903133d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Fingerprint classification\n",
    "\n",
    "This tutorial was adapted from from [Tensorflow tutorial Image Classification](https://www.tensorflow.org/tutorials/images/classification) and using the [SOCOFing data set from Kaggle](https://www.kaggle.com/datasets/ruizgara/socofing) to demonstrate different dataset, training and tuning strategies.\n",
    "\n",
    "This notebook is designed to run on SageMaker notebook instances with a conda_tensorflow2_p38 kernel.\n",
    "\n",
    "1. Prepare your environment and the data\n",
    "1. Ingest data from multiple buckets\n",
    "1. Split the data into train, validation and test\n",
    "1. Augment the data for a more robust model\n",
    "1. Build a model architecture from scratch\n",
    "1. Explore training strategies based on available resources\n",
    "1. Define hyperparameters that the model will learn for greater accuracy\n",
    "1. Train the model\n",
    "1. Evaluate the model\n",
    "1. Serialize the model in different formats in S3\n",
    "1. Save the tuning trials in S3\n",
    "1. Load the model\n",
    "1. Load a new unseen data\n",
    "1. Convert image to an array\n",
    "1. Make a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d4b84",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b296636-0a08-47ba-b960-01cfda1a1c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages and frameworks\n",
    "\n",
    "# TODO move upgrade to start script notebook lifecycle script\n",
    "!pip install -U pip --quiet\n",
    "!pip install -r ../../requirements.txt --quiet\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# debugging code \"Cleanup Called...\" gets displayed if get_logger is not set\n",
    "# the below code suppresses the \"Cleanup Called...\" output\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "# expecting 2.11\n",
    "# if 2.7, than logging errors will show \"Cleanup called...\"\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df79ba-3018-4150-b2a1-d8722577b105",
   "metadata": {},
   "source": [
    "If this is your first time running the notebook, you may need to restart the kernel after the Tensorflow upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089211a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this directory is apart of the .gitignore to ensure it is not committed to git\n",
    "%env SCRATCH=../scratch\n",
    "![ -e \"${SCRATCH}\" ] || mkdir -p \"${SCRATCH}\"/model\n",
    "\n",
    "scratch_path = os.environ.get('SCRATCH', '../scratch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b492230",
   "metadata": {},
   "source": [
    "## Create local directory structure\n",
    "\n",
    "- decompressed_data: to store the decompressed training examples\n",
    "- model: to demonstrate storing the saved models and hyperparameter trials results\n",
    "- train and test: to demonstrate download interaction with s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f6fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $SCRATCH/{train,real,model} && \\\n",
    "mkdir -p $SCRATCH/train/{left,right}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62063b13",
   "metadata": {},
   "source": [
    "## Decompress the data for training\n",
    "\n",
    "This is just a simple way to get the decompressed data into the S3 Bucket that keeps this demo and data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a1779",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xJf ./compressed_data/left.xz -C $scratch_path/decompressed_data/ && \\\n",
    "tar -xJf ./compressed_data/right.xz -C $scratch_path/decompressed_data/ && \\\n",
    "tar -xJf ./compressed_data/real.xz -C $scratch_path "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104057a",
   "metadata": {},
   "source": [
    "## Connect to S3 Buckets\n",
    "\n",
    "Boto3 allows for easy access to AWS tools and objects. We will establish the resource and list the s3 buckets available.\n",
    "\n",
    "***ACTION REQUIRED:***\n",
    "Create these Buckets from the AWS ACK Operator for S3 from the OpenShift console. Example CRs are under examples directory, for example \"demo-rosa-sagemaker/openshift/examples/s3-sagemaker-real-cr.yml\"\n",
    "\n",
    "`IMPORTANT: in order for sagemaker to write or read, the bucket names MUST include \"sagemaker-\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0168d92-cfd5-4444-b9cb-252cceaac5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boto3 library allows for easy access to aws ecosystem of tools and products. \n",
    "# SageMaker is a part of aws ecosystem of tools, so it allows easy access to S3.\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# list all available buckets\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e855be42-785d-4cc7-9352-0b7f36859322",
   "metadata": {},
   "source": [
    "### Upload training data to s3\n",
    "\n",
    "1. Upload left data from `decompressed_data` to `s3://sagemaker-left`\n",
    "1. Upload right data from `decompressed_data` to `s3://sagemaker-right`\n",
    "1. Upload real data from `decompressed_data` to `s3://sagemaker-real`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b733d1-8f8f-486a-a9e0-8264321315a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync $SCRATCH/decompressed_data/left s3://sagemaker-left --quiet && \\\n",
    "aws s3 sync $SCRATCH/decompressed_data/right s3://sagemaker-right --quiet && \\\n",
    "aws s3 sync $SCRATCH/real s3://sagemaker-real --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18047be-9dc0-4b3e-b60c-1f2dd36bdc7a",
   "metadata": {},
   "source": [
    "### Download training data from s3\n",
    "\n",
    "1. Download left data from `s3://sagemaker-left` to `train/left`\n",
    "1. Download right data from `s3://sagemaker-right` to `train/right`\n",
    "1. Download real data from `s3://sagemaker-real`  to `real`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffbc157-1ca2-43c2-8d28-04fbf1598dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data from S3 to the train and test folders\n",
    "!aws s3 sync s3://sagemaker-left $SCRATCH/train/left --quiet && \\\n",
    "aws s3 sync s3://sagemaker-right $SCRATCH/train/right --quiet && \\\n",
    "aws s3 sync s3://sagemaker-real $SCRATCH/test --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00308b1-7eca-46eb-aed9-00e52c043d41",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b906563",
   "metadata": {},
   "source": [
    "### Split the data into Train, Validation and Test\n",
    "\n",
    "Keras utility generates a dataset from image files in a directory and infers the labels based on the parent folder. This utility will return a tf.data.Dataset that yields batches of images from the subdirectories left and right\n",
    "\n",
    "```\n",
    "train/\n",
    "...left/\n",
    "......a_image_1.jpg\n",
    "......a_image_2.jpg\n",
    "...right/\n",
    "......b_image_1.jpg\n",
    "......b_image_2.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb96d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# set variables for consistency\n",
    "# \n",
    "\n",
    "img_height = 96              # desired height\n",
    "img_width = 96               # desiredd width\n",
    "batch_size = 32              # batch inputs in 32\n",
    "seed_train_validation = 42   # Must be same for train_ds and val_ds\n",
    "validation_split = 0.3       # move 30% of the data into validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd733a6",
   "metadata": {},
   "source": [
    "### Create Train\n",
    "\n",
    "Train is the sample of data used to fit the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f74933",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '../scratch/train',\n",
    "    labels='inferred',\n",
    "    label_mode = \"categorical\", \n",
    "    class_names=['left','right'],\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True, \n",
    "    seed=seed_train_validation,\n",
    "    validation_split=validation_split,\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f050970a",
   "metadata": {},
   "source": [
    "### Create Validation\n",
    "\n",
    "Validation is the sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '../scratch/train',\n",
    "    labels='inferred',\n",
    "    label_mode = \"categorical\", \n",
    "    class_names=['left','right'],\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True, \n",
    "    seed=seed_train_validation,\n",
    "    validation_split=validation_split,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82c281",
   "metadata": {},
   "source": [
    "## Create Test\n",
    "\n",
    "The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd96222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the validation_ds into validation and test data\n",
    "test_ds = validation_ds.take(16)\n",
    "validation_ds = validation_ds.skip(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d9c9f5",
   "metadata": {},
   "source": [
    "## Print the Dataset batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserves 393 batches training\n",
    "print('70% of data in batches of 32 images for training -->', train_ds.cardinality())\n",
    "# reserves 164 batches validation\n",
    "print('20% of data in batches of 32 images for validating -->', validation_ds.cardinality())\n",
    "# reserves 5 batches testing\n",
    "print('10% of data in batches of 32 images for testing -->', test_ds.cardinality())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e179d3",
   "metadata": {},
   "source": [
    "## Print Inferred Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e7c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the class names inferred from the training dataset\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c20166d",
   "metadata": {},
   "source": [
    "## Print Fingerprint Data Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adbd61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first 10 images in the training dataset\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(10):\n",
    "        ax = plt.subplot(5, 5, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"), cmap='gray')\n",
    "        #TODO update labels\n",
    "        #plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c7699e",
   "metadata": {},
   "source": [
    "## Apply augmentation\n",
    "\n",
    "When you don't have a large image dataset or when your images are all set in a single direction like ours are, it's a good practice to artificially introduce sample diversity by applying random, yet realistic, transformations to the training images, such as rotation and horizontal flipping. This helps expose the model to different aspects of the training data and reduce overfitting. Learn more https://www.tensorflow.org/tutorials/images/data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e93291",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "\n",
    "  # randomly rotates images during training\n",
    "  tf.keras.layers.RandomRotation(\n",
    "    # a float represented as fraction of 2 Pi, or a tuple of size 2 representing lower and upper bound for rotating clockwise and counter-clockwise. \n",
    "    # A positive values means rotating counter clock-wise, while a negative value means clock-wise. \n",
    "    0.2,\n",
    "      \n",
    "    # Points outside the boundaries of the input are filled according to the given mode (one of {\"constant\", \"reflect\", \"wrap\", \"nearest\"}).\n",
    "    fill_mode='constant',\n",
    "      \n",
    "    # Supported values: \"nearest\", \"bilinear\".\n",
    "    interpolation='nearest',\n",
    "      \n",
    "    # Integer. Used to create a random seed.\n",
    "    seed=None,\n",
    "      \n",
    "    # the value to be filled outside the boundaries when fill_mode=\"constant\".\n",
    "    fill_value=0.0,\n",
    "),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a748ba6",
   "metadata": {},
   "source": [
    "Visualize a few augmented examples by applying data augmentation to the same image several times:Visualize a few augmented examples by applying data augmentation to the same image several times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b89acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, _ in train_ds.take(1):\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  first_image = image[2]\n",
    "  for i in range(10):\n",
    "    ax = plt.subplot(5, 5, i + 1)\n",
    "    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "    plt.imshow(augmented_image[0] / 1, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec6df2",
   "metadata": {},
   "source": [
    "## Configure the datasets for performance\n",
    "\n",
    "Let's make sure to use buffered prefetching so we can yield data from disk without having I/O become blocking. These are two important methods you should use when loading data.\n",
    "\n",
    "1. `Caching` a dataset, either in memory or on local storage. This will save some operations (like file opening and data reading) from being executed during each epoch.\n",
    "1. `Prefetching` overlaps the preprocessing and model execution of a training step. While the model is executing training step s, the input pipeline is reading the data for step s+1. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37bc48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "validation_ds = validation_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = validation_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aa120b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6650a17",
   "metadata": {},
   "source": [
    "## Define a training strategy\n",
    "\n",
    "tf.distribute.Strategy is a TensorFlow API to distribute training across multiple GPUs, multiple machines, or TPUs. Using this API, you can distribute your existing models and training code with minimal code changes.\n",
    "- Easy to use and support multiple user segments, including researchers, machine learning engineers, etc.\n",
    "- Provide good performance out of the box.\n",
    "- Easy switching between strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f62d4-a2b6-4f5e-900a-b244576a81bf",
   "metadata": {},
   "source": [
    "### One Device\n",
    "\n",
    "A distribution strategy for running on a single device. Device string identifier for the device on which the variables should be placed. See class docs for more details on how the device is used. Examples: \"/cpu:0\", \"/gpu:0\", \"/device:CPU:0\", \"/device:GPU:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a3be45-a5e6-4124-b431-fbf5426b396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# uncomment to create a OneDeviceStrategy.\n",
    "# \n",
    "\n",
    "#strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ed9ca",
   "metadata": {},
   "source": [
    "### Mirrored Strategy \n",
    "\n",
    "Supports synchronous distributed training on multiple GPUs on one machine. It creates one replica per GPU device. Each variable in the model is mirrored across all the replicas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda4949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# uncomment to create a MirroredStrategy.\n",
    "# \n",
    "\n",
    "#strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "#strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a45f5",
   "metadata": {},
   "source": [
    "### Multi-Worker Mirrored Strategy\n",
    "\n",
    "Multi-Worker MirroredStrategy is very similar to MirroredStrategy. It implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to tf.distribute.MirroredStrategy, it creates copies of all variables in the model on each device across all workers.\n",
    "\n",
    "MultiWorkerMirroredStrategy has two implementations for cross-device communications. \n",
    "1. CommunicationImplementation.RING is RPC-based and supports both CPUs and GPUs. \n",
    "1. CommunicationImplementation.NCCL uses NCCL and provides state-of-art performance on GPUs but it doesn't support CPUs. \n",
    "1. CollectiveCommunication.AUTO defers the choice to Tensorflow. You can specify them in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b673caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# uncomment to create a Multi-worker Mirrored Strategy\n",
    "#\n",
    "\n",
    "communication_options = tf.distribute.experimental.CommunicationOptions(\n",
    "    # RING is RPC-based and supports both CPUs and GPUs.\n",
    "    #implementation=tf.distribute.experimental.CommunicationImplementation.RING)\n",
    "\n",
    "    # NCCL uses NCCL and provides state-of-art performance on GPUs but it doesn't support CPUs\n",
    "    #implementation=tf.distribute.experimental.CommunicationImplementation.NCCL)\n",
    "    \n",
    "    # AUTO defers the choice to Tensorflow.\n",
    "    implementation=tf.distribute.experimental.CommunicationImplementation.AUTO)\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=communication_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9cad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Understand the number of devices and GPU available\n",
    "#\n",
    "\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88fe6ca",
   "metadata": {},
   "source": [
    "## Define a model and hyperparameters\n",
    "\n",
    "When you build a model for hypertuning, you also define the hyperparameter search space in addition to the model architecture. The model you set up for hypertuning is called a hypermodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb97134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# A model definition that doesn't take advantage of a training strategy\n",
    "#\n",
    "!pip install keras-tuner -q\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout,Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "\n",
    "num_classes = len(class_names)\n",
    "model_path = scratch_path + '/model'\n",
    "\n",
    "inputShape=(img_height, img_width, 1)\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    data_augmentation\n",
    "    input_shape=(img_height, img_width, 1)\n",
    "    chanDim = -1\n",
    "    # first CONV => RELU => POOL layer set\n",
    "    model.add(Conv2D(\n",
    "        hp.Int(\"conv_1\", min_value=32, max_value=96, step=32),\n",
    "        (3, 3), padding=\"same\", input_shape=inputShape, data_format=\"channels_last\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # second CONV => RELU => POOL layer set\n",
    "    model.add(Conv2D(\n",
    "        hp.Int(\"conv_2\", min_value=64, max_value=128, step=32),\n",
    "        (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # third CONV => RELU => POOL layer set\n",
    "    model.add(Conv2D(\n",
    "        hp.Int(\"conv_3\", min_value=96, max_value=256, step=32),\n",
    "        (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))    \n",
    "    \n",
    "    # first (and only) set of FC => RELU layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hp.Int(\"dense_units\", min_value=256,\n",
    "                           max_value=768, step=256)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    # softmax classifier\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    # initialize the learning rate choices and optimizer\n",
    "    lr = hp.Choice(\"learning_rate\",\n",
    "                   values=[1e-1, 1e-2, 1e-3])\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    # return the model\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        # metrics to be evaluated by the model during training and testing.The strings 'accuracy' or 'acc', TF converts this to binary, categorical or sparse.\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16b28b8",
   "metadata": {},
   "source": [
    "## Search hyperparameters\n",
    "\n",
    "The Keras Tuner has four tuners available:\n",
    "1. RandomSearch\n",
    "1. Hyperband\n",
    "1. BayesianOptimization\n",
    "1. Sklearn. \n",
    "\n",
    "In this tutorial, you use the Hyperband tuner. The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. \n",
    "\n",
    "To instantiate the Hyperband tuner, you must specify the hypermodel, the objective to optimize and the maximum number of epochs to train (max_epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a736edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "model_path = scratch_path + '/model'\n",
    "\n",
    "# Open a strategy scope.\n",
    "tuner = kt.Hyperband(\n",
    "    model_builder,\n",
    "    objective='val_accuracy',\n",
    "    # Unteger, the maximum number of epochs to train one model. It is recommended to set this to a value slightly higher than the expected epochs to convergence for your largest Model, and to use early stopping during trainin\n",
    "    max_epochs=5,\n",
    "    # Integer, the reduction factor for the number of epochs and number of models for each bracket. Defaults to 3.\n",
    "    factor=3,\n",
    "    # training strategy\n",
    "    distribution_strategy=strategy,\n",
    "    # directory to save the hyperparameter trials\n",
    "    directory=scratch_path + '/model/model_hp',\n",
    "    # folder to save the hyperparameter trail outputs\n",
    "    project_name='hypertune',\n",
    "    #  If you re-run the hyperparameter search, the Keras Tuner uses the existing state from these logs to resume the search. \n",
    "    # To disable this behavior, pass an additional overwrite=True argument while instantiating the tuner.\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12faf8",
   "metadata": {},
   "source": [
    "We’ll be using EarlyStopping to short circuit hyperparameter trials that are not performing well. Keep in mind that tuning hyperparameters is an extremely computationally expensive process, so if we can kill off poorly performing trials, we can save ourselves a bunch of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e88ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop training when a monitored metric has stopped improving.\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b0fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_ds, epochs=4, validation_data=validation_ds, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"[INFO] optimal number of filters in conv_1 layer: {}\".format(\n",
    "\tbest_hps.get(\"conv_1\")))\n",
    "print(\"[INFO] optimal number of filters in conv_2 layer: {}\".format(\n",
    "\tbest_hps.get(\"conv_2\")))\n",
    "print(\"[INFO] optimal number of filters in conv_3 layer: {}\".format(\n",
    "\tbest_hps.get(\"conv_3\")))\n",
    "print(\"[INFO] optimal number of units in dense layer: {}\".format(\n",
    "\tbest_hps.get(\"dense_units\")))\n",
    "print(\"[INFO] optimal learning rate: {:.4f}\".format(\n",
    "\tbest_hps.get(\"learning_rate\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b91ead",
   "metadata": {},
   "source": [
    "## Fit a model\n",
    "\n",
    "Fit the model with the optimal hyperparameters and train it on the data for a desired number of epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88925c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 9\n",
    "\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=validation_ds,\n",
    "    epochs=epochs,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False\n",
    ")\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3ba56",
   "metadata": {},
   "source": [
    "## Print the model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c3ef9b",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8674afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(\n",
    "    test_ds,\n",
    "    batch_size=batch_size,\n",
    "    verbose='auto',\n",
    "    sample_weight=None,\n",
    "    steps=None,\n",
    "    callbacks=None,\n",
    "    max_queue_size=10,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe4249",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "\n",
    "You can save an entire model to a single artifact. It will include:\n",
    "\n",
    "- The model's architecture/config\n",
    "- The model's weight values (which were learned during training)\n",
    "- The model's compilation information (if compile() was called)\n",
    "- The optimizer and its state, if any (this enables you to restart training where you left)\n",
    "\n",
    "There are two formats you can use to save an entire model to disk: \n",
    "1. the TensorFlow SavedModel format\n",
    "1. the older Keras H5 format.\n",
    "\n",
    "For versioning, you typically generate several models made up of (code, data, config) that demands model versioning. Toyday there is:\n",
    "\n",
    "1. no uniform standard accepted\n",
    "1. different organizations have different conventions\n",
    "\n",
    "MAJOR.MINOR.PIPELINE\n",
    "\n",
    "MAJOR: increases when incompatible data changes, schema or target variable change, that renders model incompatible when used for predictions\n",
    "MINOR: increases when model performance is improved\n",
    "PIPELINE: correspond to an update to training pipeline but may not change the model itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dbdc54",
   "metadata": {},
   "source": [
    "## Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36539eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# older Keras H5 format\n",
    "model.save( scratch_path + '/model/left-right-print-prediction-1.0.0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b256bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow SavedModel format\n",
    "model.save( scratch_path + '/model/left-right-print-prediction-1.0.0.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60023558",
   "metadata": {},
   "source": [
    "## S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14928535-7f5a-4fa5-8b6b-4e0aa7c138ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models to S3\n",
    "!aws s3 sync $SCRATCH/model/ s3://sagemaker-fingerprint-model-lr --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc59007",
   "metadata": {},
   "source": [
    "# Make a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc36c87",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e90e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model( scratch_path + '/model/left-right-print-prediction-1.0.0.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f046e7",
   "metadata": {},
   "source": [
    "## Load a new fingerprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc589378",
   "metadata": {},
   "source": [
    "## Convert the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3a384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# un/comment test a print\n",
    "#path = scratch_path + '/real/1__M_Right_ring_finger.png'\n",
    "path = scratch_path + '/real/380__M_Right_index_finger.png'\n",
    "#path = scratch_path + '/real/466__F_Right_thumb_finger.png'\n",
    "#path = scratch_path + '/real/1__M_Left_little_finger.png'\n",
    "#path = scratch_path + '/real/105__M_Left_little_finger.png'\n",
    "#path = scratch_path + '/real/467__M_Left_little_finger.png'\n",
    "\n",
    "# Loads an image into PIL format.\n",
    "img = tf.keras.utils.load_img(\n",
    "    path,\n",
    "    color_mode=\"grayscale\",\n",
    "    target_size=(img_height, img_width),\n",
    "    interpolation='nearest',\n",
    ")\n",
    "\n",
    "plt.imshow(img, cmap='gray')\n",
    "\n",
    "# Converts a PIL Image instance to a Numpy array.\n",
    "img_array = tf.keras.utils.img_to_array(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcc51c6",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c084f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a prediction on the new fingerprint\n",
    "predictions = model.predict(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fa656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acbffbe-083e-4076-a5a9-6807998b34b9",
   "metadata": {},
   "source": [
    "# Cleanup artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7417ec1c-dfc0-4f2f-80b8-0a0944bc0e78",
   "metadata": {},
   "source": [
    "## Delete local directory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de13571-bdbd-4e94-9e5d-9fe39eda9090",
   "metadata": {},
   "outputs": [],
   "source": [
    "break\n",
    "!rm -rf $scratch_path/decompressed_data && \\\n",
    "rm -rf $scratch_path/train && \\\n",
    "rm -rf $scratch_path/real && \\\n",
    "rm -rf $scratch_path/model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Dec  7 2022, 01:36:11) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fbaf16404a186558c5830ba9c1870690eb62baaca2b916af320bab80c6b3e76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
