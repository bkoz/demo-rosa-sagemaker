{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d903133d",
   "metadata": {},
   "source": [
    "# Fingerprint classification\n",
    "\n",
    "This tutorial was adapted from from [Tensorflow tutorial Image Classification](https://www.tensorflow.org/tutorials/images/classification) and using the [SOCOFing data set from Kaggle](https://www.kaggle.com/datasets/ruizgara/socofing) to demonstrate different dataset, training and tuning strategies.\n",
    "\n",
    "This notebook is designed to run on SageMaker notebook instances with a conda_tensorflow2_p38 kernel.\n",
    "\n",
    "- Examine and understand data\n",
    "- Identifying overfitting and applying techniques to mitigate it, including data augmentation and dropout.\n",
    "- Efficiently loading a dataset off disk.\n",
    "- Build an input pipeline\n",
    "- Build the model\n",
    "- Train the model\n",
    "- Test the model\n",
    "- Improve the model and repeat the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cd4936-7aa3-48c8-b4f3-b5764c5f20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow -q\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d4b84",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089211a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this directory is apart of the .gitignore to ensure it is not committed to git\n",
    "%env SCRATCH=../scratch\n",
    "![ -e \"${SCRATCH}\" ] || mkdir -p \"${SCRATCH}\"/model\n",
    "\n",
    "import os\n",
    "scratch_path = os.environ.get('SCRATCH', '../scratch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193fb493",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b492230",
   "metadata": {},
   "source": [
    "## Create local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f6fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $scratch_path/hand && $scratch_path/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62063b13",
   "metadata": {},
   "source": [
    "## Extract the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a1779",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xJf ./compressed_data/left.xz -C $scratch_path/hand/ && \\\n",
    "tar -xJf ./compressed_data/right.xz -C ../scratch/hand/ && \\\n",
    "tar -xJf ./compressed_data/real.xz -C ../scratch/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b906563",
   "metadata": {},
   "source": [
    "## Split the data into Train, Validation and Test\n",
    "\n",
    "Keras utility generates a dataset from image files in a directory and infers the labels based on the parent folder. This utility will return a tf.data.Dataset that yields batches of images from the subdirectories left and right\n",
    "\n",
    "```\n",
    "/scratch/hand/\n",
    "...left/\n",
    "......a_image_1.jpg\n",
    "......a_image_2.jpg\n",
    "...right/\n",
    "......b_image_1.jpg\n",
    "......b_image_2.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb96d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# set variables for consistency\n",
    "# \n",
    "\n",
    "img_height = 96              # desired image height\n",
    "img_width = 96               # desiredd image width\n",
    "batch_size = 32              # batches of 32 images to process at a time\n",
    "seed_train_validation = 42   # Must be same for train_ds and val_ds\n",
    "validation_split = 0.3       # move 30% of the data into validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd733a6",
   "metadata": {},
   "source": [
    "### Create Train\n",
    "\n",
    "Train is the sample of data used to fit the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f74933",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '../scratch/hand',\n",
    "    labels='inferred',\n",
    "    label_mode = \"categorical\", \n",
    "    class_names=['left','right'],\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True, \n",
    "    seed=seed_train_validation,\n",
    "    validation_split=validation_split,\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f050970a",
   "metadata": {},
   "source": [
    "### Create Validation\n",
    "\n",
    "Validation is the sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '../scratch/hand',\n",
    "    labels='inferred',\n",
    "    label_mode = \"categorical\", \n",
    "    class_names=['left','right'],\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True, \n",
    "    seed=seed_train_validation,\n",
    "    validation_split=validation_split,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82c281",
   "metadata": {},
   "source": [
    "## Create Test\n",
    "\n",
    "The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd96222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the validation_ds into validation and test data\n",
    "test_ds = validation_ds.take(16)\n",
    "validation_ds = validation_ds.skip(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d9c9f5",
   "metadata": {},
   "source": [
    "## Print the Dataset batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserves 393 batches training\n",
    "print('70% of data in batches of 32 images for training -->', train_ds.cardinality())\n",
    "# reserves 164 batches validation\n",
    "print('20% of data in batches of 32 images for validating -->', validation_ds.cardinality())\n",
    "# reserves 5 batches testing\n",
    "print('10% of data in batches of 32 images for testing -->', test_ds.cardinality())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e179d3",
   "metadata": {},
   "source": [
    "## Print Inferred Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e7c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the class names inferred from the training dataset\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c20166d",
   "metadata": {},
   "source": [
    "## Print Fingerprint Data Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adbd61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first 10 images in the training dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(10):\n",
    "    ax = plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"), cmap='gray')\n",
    "    #TODO update labels\n",
    "    #plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c7699e",
   "metadata": {},
   "source": [
    "## Apply augmentation\n",
    "\n",
    "When you don't have a large image dataset or when your images are all set in a single direction like ours are, it's a good practice to artificially introduce sample diversity by applying random, yet realistic, transformations to the training images, such as rotation and horizontal flipping. This helps expose the model to different aspects of the training data and reduce overfitting. Learn more https://www.tensorflow.org/tutorials/images/data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e93291",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "\n",
    "  # randomly rotates images during training\n",
    "  tf.keras.layers.RandomRotation(\n",
    "    # a float represented as fraction of 2 Pi, or a tuple of size 2 representing lower and upper bound for rotating clockwise and counter-clockwise. \n",
    "    # A positive values means rotating counter clock-wise, while a negative value means clock-wise. \n",
    "    0.2,\n",
    "      \n",
    "    # Points outside the boundaries of the input are filled according to the given mode (one of {\"constant\", \"reflect\", \"wrap\", \"nearest\"}).\n",
    "    fill_mode='constant',\n",
    "      \n",
    "    # Supported values: \"nearest\", \"bilinear\".\n",
    "    interpolation='nearest',\n",
    "      \n",
    "    # Integer. Used to create a random seed.\n",
    "    seed=None,\n",
    "      \n",
    "    # the value to be filled outside the boundaries when fill_mode=\"constant\".\n",
    "    fill_value=0.0,\n",
    "),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a748ba6",
   "metadata": {},
   "source": [
    "Visualize a few augmented examples by applying data augmentation to the same image several times:Visualize a few augmented examples by applying data augmentation to the same image several times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b89acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, _ in train_ds.take(1):\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  first_image = image[2]\n",
    "  for i in range(10):\n",
    "    ax = plt.subplot(5, 5, i + 1)\n",
    "    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "    plt.imshow(augmented_image[0] / 1, cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec6df2",
   "metadata": {},
   "source": [
    "## Configure the datasets for performance\n",
    "\n",
    "Let's make sure to use buffered prefetching so we can yield data from disk without having I/O become blocking. These are two important methods you should use when loading data.\n",
    "\n",
    "1. `Caching` a dataset, either in memory or on local storage. This will save some operations (like file opening and data reading) from being executed during each epoch.\n",
    "1. `Prefetching` overlaps the preprocessing and model execution of a training step. While the model is executing training step s, the input pipeline is reading the data for step s+1. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37bc48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "validation_ds = validation_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = validation_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aa120b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6650a17",
   "metadata": {},
   "source": [
    "## Define a training strategy\n",
    "\n",
    "tf.distribute.Strategy is a TensorFlow API to distribute training across multiple GPUs, multiple machines, or TPUs. Using this API, you can distribute your existing models and training code with minimal code changes.\n",
    "- Easy to use and support multiple user segments, including researchers, machine learning engineers, etc.\n",
    "- Provide good performance out of the box.\n",
    "- Easy switching between strategies.\n",
    "\n",
    "Distributed training strategies intend to cover a number of use cases along different axes, including:\n",
    "\n",
    "*Synchronous vs asynchronous training: These are two common ways of distributing training with data parallelism.*\n",
    "1. Synchronous - In sync training, all workers train over different slices of input data in sync, and aggregating gradients at each step. Typically sync training is supported via all-reduce \n",
    "2. Asynchronous - In async training, all workers are independently training over the input data and updating variables asynchronously. Typically async training is supported via parameter server architecture.\n",
    "\n",
    "*Hardware platform: You may want to scale your training onto multiple GPUs on one machine, or multiple machines in a network (with 0 or more GPUs each), or on Cloud TPUs.*\n",
    "1. MirroredStrategy - shown below\n",
    "1. MultiWorkerMirroredStrategy - shown below\n",
    "1. TPUStrategy\n",
    "1. ParameterServerStrategy\n",
    "1. CentralStorageStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4405b38c-141b-404f-a452-87a044286af2",
   "metadata": {},
   "source": [
    "## Return a list of physical devices visible to the host runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4104b49-a25c-44c0-865d-00494f652cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs:\", len(physical_devices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28f32b2-0adc-4e6c-8b50-3137ec6db19e",
   "metadata": {},
   "source": [
    "### One Device Strategy\n",
    "\n",
    "A distribution strategy for running on a single device. Using this strategy will place any variables created in its scope on the specified device. Input distributed through this strategy will be prefetched to the specified device. Typical usage of this strategy could be testing your code with the tf.distribute.Strategy API before switching to other strategies which actually distribute to multiple devices/machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74bc68e-7e88-46ba-b02f-75b00248bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# uncomment to create a OneDeviceStrategy.\n",
    "# \n",
    "\n",
    "strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ed9ca",
   "metadata": {},
   "source": [
    "### Mirrored Strategy \n",
    "\n",
    "Supports synchronous distributed training on multiple GPUs on one machine. It creates one replica per GPU device. Each variable in the model is mirrored across all the replicas. Efficient all-reduce algorithms are used to communicate the variable updates across the devices. All-reduce aggregates tensors across all the devices by adding them up, and makes them available on each device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda4949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# uncomment to create a MirroredStrategy.\n",
    "# \n",
    "\n",
    "#strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "#strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a45f5",
   "metadata": {},
   "source": [
    "### Multi-Worker Mirrored Strategy\n",
    "\n",
    "Multi-Worker MirroredStrategy is very similar to MirroredStrategy. It implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to tf.distribute.MirroredStrategy, it creates copies of all variables in the model on each device across all workers.\n",
    "\n",
    "MultiWorkerMirroredStrategy has two implementations for cross-device communications. \n",
    "1. CommunicationImplementation.RING is RPC-based and supports both CPUs and GPUs. \n",
    "1. CommunicationImplementation.NCCL uses NCCL and provides state-of-art performance on GPUs but it doesn't support CPUs. \n",
    "1. CollectiveCommunication.AUTO defers the choice to Tensorflow. You can specify them in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b673caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# uncomment to create a Multi-worker Mirrored Strategy\n",
    "#\n",
    "\n",
    "#communication_options = tf.distribute.experimental.CommunicationOptions(\n",
    "    # RING is RPC-based and supports both CPUs and GPUs.\n",
    "    #implementation=tf.distribute.experimental.CommunicationImplementation.RING)\n",
    "\n",
    "    # NCCL uses NCCL and provides state-of-art performance on GPUs but it doesn't support CPUs\n",
    "    #implementation=tf.distribute.experimental.CommunicationImplementation.NCCL)\n",
    "    \n",
    "    # AUTO defers the choice to Tensorflow.\n",
    "#    implementation=tf.distribute.experimental.CommunicationImplementation.AUTO)\n",
    "#strategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=communication_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9cad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Understand the number of devices and GPU available\n",
    "#\n",
    "\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88fe6ca",
   "metadata": {},
   "source": [
    "## Define a model and hyperparameters\n",
    "\n",
    "When you build a model for hypertuning, you also define the hyperparameter search space in addition to the model architecture. The model you set up for hypertuning is called a hypermodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb97134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# A model definition that doesn't take advantage of a training strategy\n",
    "#\n",
    "\n",
    "!pip install keras-tuner -q\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout,Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "\n",
    "num_classes = len(class_names)\n",
    "model_path = scratch_path + '/model'\n",
    "\n",
    "inputShape=(img_height, img_width, 1)\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    data_augmentation\n",
    "    input_shape=(img_height, img_width, 1)\n",
    "    chanDim = -1\n",
    "    # first CONV => RELU => POOL layer set\n",
    "    model.add(Conv2D(\n",
    "        hp.Int(\"conv_1\", min_value=32, max_value=96, step=32),\n",
    "        (3, 3), padding=\"same\", input_shape=inputShape, data_format=\"channels_last\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # second CONV => RELU => POOL layer set\n",
    "    model.add(Conv2D(\n",
    "        hp.Int(\"conv_2\", min_value=64, max_value=128, step=32),\n",
    "        (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # third CONV => RELU => POOL layer set\n",
    "    model.add(Conv2D(\n",
    "        hp.Int(\"conv_3\", min_value=96, max_value=256, step=32),\n",
    "        (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))    \n",
    "    \n",
    "    # first (and only) set of FC => RELU layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hp.Int(\"dense_units\", min_value=256,\n",
    "                           max_value=768, step=256)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    # softmax classifier\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    # initialize the learning rate choices and optimizer\n",
    "    lr = hp.Choice(\"learning_rate\",\n",
    "                   values=[1e-1, 1e-2, 1e-3])\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    # return the model\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        # metrics to be evaluated by the model during training and testing.The strings 'accuracy' or 'acc', TF converts this to binary, categorical or sparse.\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16b28b8",
   "metadata": {},
   "source": [
    "## Search hyperparameters\n",
    "\n",
    "The Keras Tuner has four tuners available:\n",
    "1. Hyperband - shown below\n",
    "1. RandomSearch\n",
    "1. BayesianOptimization\n",
    "1. Sklearn. \n",
    "\n",
    "In this tutorial, you use the Hyperband tuner. The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. \n",
    "\n",
    "To instantiate the Hyperband tuner, you must specify the hypermodel, the objective to optimize and the maximum number of epochs to train (max_epochs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca5ae25-ff0f-4ff2-abe2-b958d177462f",
   "metadata": {},
   "source": [
    "### Hyperband Tuner\n",
    "\n",
    "Hyperband is a relatively new method for tuning iterative algorithms. It performs random sampling and attempts to gain an edge by using time spent optimizing in the best way. [FastML](http://fastml.com/tuning-hyperparams-fast-with-hyperband/#:~:text=Hyperband%20is%20a%20relatively%20new,optimizing%20in%20the%20best%20way.)\n",
    "\n",
    "Li, Lisha, and Kevin Jamieson. [\"Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization.\" Journal of Machine Learning Research 18 (2018): 1-52](https://jmlr.org/papers/v18/16-558.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a736edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "model_path = scratch_path + '/model'\n",
    "\n",
    "# Open a strategy scope.\n",
    "tuner = kt.Hyperband(\n",
    "    model_builder,\n",
    "    objective='val_accuracy',\n",
    "    # Integer, the maximum number of epochs to train one model. It is recommended to set this to a value slightly higher than the expected epochs to convergence for your largest Model, and to use early stopping during trainin\n",
    "    max_epochs=2,\n",
    "    # Integer, the reduction factor for the number of epochs and number of models for each bracket. Defaults to 3.\n",
    "    factor=3,\n",
    "    # training strategy\n",
    "    distribution_strategy=strategy,\n",
    "    directory=scratch_path + '/model/model_hp',\n",
    "    project_name='hypertune',\n",
    "    #  If you re-run the hyperparameter search, the Keras Tuner uses the existing state from these logs to resume the search. \n",
    "    # To disable this behavior, pass an additional overwrite=True argument while instantiating the tuner.\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03a340d-9a02-4aa4-83f9-3d1308879f80",
   "metadata": {},
   "source": [
    "### RandomSearch Tuner\n",
    "\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    model_builder,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    seed=None,\n",
    "    hyperparameters=None,\n",
    "    tune_new_entries=True,\n",
    "    allow_new_entries=True,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12faf8",
   "metadata": {},
   "source": [
    "We’ll be using EarlyStopping to short circuit hyperparameter trials that are not performing well. Keep in mind that tuning hyperparameters is an extremely computationally expensive process, so if we can kill off poorly performing trials, we can save ourselves a bunch of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e88ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop training when a monitored metric has stopped improving.\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b0fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_ds, epochs=4, validation_data=validation_ds, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"[INFO] optimal number of filters in conv_1 layer: {}\".format(\n",
    "\tbest_hps.get(\"conv_1\")))\n",
    "print(\"[INFO] optimal number of filters in conv_2 layer: {}\".format(\n",
    "\tbest_hps.get(\"conv_2\")))\n",
    "print(\"[INFO] optimal number of filters in conv_2 layer: {}\".format(\n",
    "\tbest_hps.get(\"conv_3\")))\n",
    "print(\"[INFO] optimal number of units in dense layer: {}\".format(\n",
    "\tbest_hps.get(\"dense_units\")))\n",
    "print(\"[INFO] optimal learning rate: {:.4f}\".format(\n",
    "\tbest_hps.get(\"learning_rate\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b91ead",
   "metadata": {},
   "source": [
    "## Fit a model\n",
    "\n",
    "Fit the model with the optimal hyperparameters and train it on the data for 50 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88925c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=validation_ds,\n",
    "    epochs=epochs,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False\n",
    ")\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3ba56",
   "metadata": {},
   "source": [
    "## Print the model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c3ef9b",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8674afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(\n",
    "    test_ds,\n",
    "    batch_size=batch_size,\n",
    "    verbose='auto',\n",
    "    sample_weight=None,\n",
    "    steps=None,\n",
    "    callbacks=None,\n",
    "    max_queue_size=10,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe4249",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "\n",
    "There are two formats you can use to save an entire model to disk: the TensorFlow SavedModel format, and the older Keras H5 format. The recommended format is SavedModel. It is the default when you use model.save()\n",
    "\n",
    "A Keras model consists of multiple components:\n",
    "\n",
    "1. The architecture, or configuration, which specifies what layers the model contain, and how they're connected.\n",
    "1. A set of weights values (the \"state of the model\").\n",
    "1. An optimizer (defined by compiling the model).\n",
    "1. A set of losses and metrics (defined by compiling the model or calling add_loss() or add_metric()).\n",
    "\n",
    "The Keras API makes it possible to save all of these pieces to disk at once, or to only selectively save some of them:\n",
    "\n",
    "1. Saving everything into a single archive in the TensorFlow SavedModel format (or in the older Keras H5 format). This is the standard practice.\n",
    "1. Saving the architecture / configuration only, typically as a JSON file.\n",
    "1. Saving the weights values only. This is generally used when training the model.\n",
    "\n",
    "There are two formats you can use to save an entire model to disk: the TensorFlow SavedModel format, and the older Keras H5 format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dbdc54",
   "metadata": {},
   "source": [
    "## Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24493d00-abfa-493c-ac88-69b454d101e3",
   "metadata": {},
   "source": [
    "### Saved Model Format\n",
    "\n",
    "A SavedModel contains a complete TensorFlow program, including trained parameters (i.e, tf.Variables) and computation. It does not require the original model building code to run, which makes it useful for sharing or deploying with TFLite, TensorFlow.js, TensorFlow Serving, or TensorFlow Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b256bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow SavedModel format\n",
    "model.save( scratch_path + '/model/hand_prediction.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf000e1-e1f2-4d27-ac24-8210264c02f6",
   "metadata": {},
   "source": [
    "### HDF5\n",
    "\n",
    "Keras also supports saving a single HDF5 file containing the model's architecture, weights values, and compile() information. It is a light-weight alternative to SavedModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36539eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# older Keras H5 format\n",
    "model.save( scratch_path + '/model/hand_prediction.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60023558",
   "metadata": {},
   "source": [
    "## S3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc59007",
   "metadata": {},
   "source": [
    "# Make a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc36c87",
   "metadata": {},
   "source": [
    "## Load the SavedModel model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e90e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model( scratch_path + '/model/hand_prediction.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f046e7",
   "metadata": {},
   "source": [
    "### Load a new fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35194285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# un/comment test a left finger\n",
    "#path = scratch_path + '/real/10__M_Left_thumb_finger.png'\n",
    "\n",
    "# un/comment test a right finger\n",
    "path = scratch_path + '/real/1__M_Right_ring_finger.png'\n",
    "\n",
    "\n",
    "# Loads an image into PIL format.\n",
    "img = tf.keras.utils.load_img(\n",
    "    path,\n",
    "    color_mode='grayscale',\n",
    "    target_size=(img_height, img_width),\n",
    "    interpolation='nearest',\n",
    "    keep_aspect_ratio=False\n",
    ")\n",
    "\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc589378",
   "metadata": {},
   "source": [
    "### Convert the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3a384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a PIL Image instance to a Numpy array.\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "\n",
    "# Returns a tensor with a length 1 axis inserted at index axis.\n",
    "img_array = tf.expand_dims(img_array, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcc51c6",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c084f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a prediction on the new fingerprint\n",
    "predictions = model.predict(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fa656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad81f17-c196-4cbb-90ab-3f5fb54472a2",
   "metadata": {},
   "source": [
    "## Load the HDF5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fcf006-7777-46f1-be77-475131369d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can be used to reconstruct the model identically.\n",
    "h5_model = keras.models.load_model( scratch_path + '/model/hand_prediction.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b938bca6-3521-4c3f-8d13-331de4616952",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734f3dd2-05f1-4abe-ab68-b90b26d757f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a prediction on the new fingerprint\n",
    "h5_predictions = reconstructed_model.predict(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37537bf4-ac40-4b51-9e5e-5ad68b8c5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = tf.nn.softmax(h5_predictions[0])\n",
    "\n",
    "print(\n",
    "    \n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe2e00-9e9e-4b9e-8f45-fede23af8e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
